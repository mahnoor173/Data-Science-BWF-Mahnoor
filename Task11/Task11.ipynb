{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning**\n"
      ],
      "metadata": {
        "id": "kVFvn3_PZW0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning:**\n",
        "Machine learning is a subset of artificial intelligence that involves the development of algorithms that allow computers to learn from and make predictions or decisions based on data. It enables systems to improve their performance on a specific task through experience without being explicitly programmed."
      ],
      "metadata": {
        "id": "07Aq2wuAZirv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised learning vs Unsupervised learning**\n",
        " Supervised learning is a type of machine learning where the algorithm is trained on labeled data. This means that each training example is paired with an output label. The goal is to learn a mapping from inputs to outputs that can be used to predict the labels of new, unseen examples.Common tasks: classification and regression.\n",
        " Definition: Unsupervised learning is a type of machine learning where the algorithm is trained on unlabeled data. The goal is to find hidden patterns or intrinsic structures in the input data. Common Tasks: Clustering and Dimensionality"
      ],
      "metadata": {
        "id": "wy1jeFk5Zan7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**validation set**\n",
        "The purpose of a validation set is to provide an unbiased evaluation of a model fit during the training phase. It is used to tune hyperparameters and make decisions about the model architecture. By using a validation set, you can prevent overfitting to the training data and ensure that the model generalizes well to new, unseen data."
      ],
      "metadata": {
        "id": "CFv7VB7nc_sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repeated cross-validation**\n",
        "Repeated cross-validation is a technique where the cross-validation process is performed multiple times with different random splits of the data. For each split, the data is divided into different training and validation sets, and the model is trained and evaluated multiple times. The results are then averaged to provide a more robust estimate of the model's performance"
      ],
      "metadata": {
        "id": "jb98RxxfdoNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Creating a Validation set**"
      ],
      "metadata": {
        "id": "Fe0rHZWEd3z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training+validation set and test set\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training+validation set into training set and validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3siGtPHgeExW",
        "outputId": "01dcca2b-a7dd-458c-ca76-a709070a55fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 90\n",
            "Validation set size: 30\n",
            "Test set size: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression Example**"
      ],
      "metadata": {
        "id": "TIof89hSeb__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic data\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear regression model\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = lin_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X_test, y_pred, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wybaJGyjegjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision tree example**"
      ],
      "metadata": {
        "id": "vL09EHQzeigT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree model\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "id": "2AlnRLQ1eqhX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}